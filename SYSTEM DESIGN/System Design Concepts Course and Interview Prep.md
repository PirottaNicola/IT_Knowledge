# CI/CD

- CI/CD is a method to frequently deliver apps to customers by introducing automation intounderwhelmed the stages of app development. The main concepts attributed to CI/CD are continuous integration, continuous delivery, and continuous deployment.
- Continuous Integration (CI) is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily - leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.
- Continuous Delivery (CD) is an extension of continuous integration to make sure that you can release new changes to your customers quickly in a sustainable way. This means that on top of having automated your testing, you also have automated your release process and you can deploy your application at any point of time by clicking on a button.

## CI/CD Pipeline

- A CI/CD pipeline automates your software delivery process. The pipeline builds code, runs tests (CI), and safely deploys a new version of the application (CD). In a CI/CD pipeline, each phase is triggered automatically by the previous one. The pipeline is created in a way that allows code to move smoothly from development to production, while ensuring that the code is always in a deployable state.

## CI/CD Tools

- Jenkins is an open-source automation server that helps to automate the parts of software development related to building, testing, and delivering, facilitating continuous integration and continuous delivery.
- Github Actions is a CI/CD tool that allows you to automate your workflow. You can write individual tasks, called actions, and combine them to create a custom workflow. Workflows are custom automated processes that you can set up in your repository to build, test, package, release, and deploy your code.

## CI/CD Best Practices

- **Automate Everything**: Automate as much as possible, including testing, building, and deployment.
- **Commit Code Frequently**: Committing code frequently helps to avoid merge conflicts and makes it easier to integrate changes.
- **Test Early and Often**: Test your code as early as possible and as often as possible to catch bugs early.
- **Deploy in Small Batches**: Deploying in small batches reduces the risk of errors and makes it easier to identify and fix issues.
- **Monitor and Measure**: Monitor your CI/CD pipeline and measure key metrics to identify bottlenecks and areas for improvement.
- **Feedback Loop**: Establish a feedback loop to gather input from developers, testers, and users to continuously improve the CI/CD process.

# Web Servers

- A web server is a software application that serves HTTP content to web browsers. It listens for incoming requests on specific TCP ports and responds with HTML content, images, or other resources.
- Web servers are used to host websites, web applications, and other content that can be accessed over the internet. They can serve static content, such as HTML files and images, as well as dynamic content generated by server-side scripts.
- Some popular web servers include Apache, Nginx, and Microsoft Internet Information Services (IIS). These servers are widely used in production environments to host websites and web applications.

## Web servers features

- **HTTP Protocol Support**: Web servers support the HTTP protocol, which is used to transfer data between the server and the client.
- **Virtual Hosting**: Web servers support virtual hosting, which allows multiple websites to be hosted on the same server.
- **Security**: Web servers provide security features such as SSL/TLS encryption, access control, and logging to protect websites and web applications.
- **Load Balancing**: Web servers can be configured to distribute incoming requests across multiple servers to improve performance and reliability.
- **Caching**: Web servers can cache static content to reduce load times and improve performance.
- **Logging**: Web servers log information about incoming requests, errors, and other events to help administrators monitor and troubleshoot server activity.

## Web servers vs Application servers

- Web servers are designed to serve static content, such as HTML files and images, over the internet. They are optimized for handling HTTP requests and responding with content quickly and efficiently.
- Application servers are designed to run server-side applications, such as web applications and APIs. They provide additional features, such as support for server-side scripting languages, database connectivity, and session management.
- some web servers, such as Apache and Nginx, can also function as application servers by supporting server-side scripting languages and other features. However, dedicated application servers, such as Tomcat and JBoss, are specifically designed to run server-side applications.
- list of web servers: Apache, Nginx, IIS, Lighttpd, Caddy, etc.
- list of application servers: Tomcat, JBoss, WebSphere, WebLogic, GlassFish, etc.

# Logging and Monitoring

- Logging is the process of recording events, actions, and status messages that occur during the operation of a software application or system. Logs are used to track and troubleshoot issues, monitor performance, and analyze trends over time.
- Monitoring is the process of observing and measuring the performance and availability of a software application or system. Monitoring tools collect data on key metrics, such as response time, CPU usage, and error rates, and provide alerts when thresholds are exceeded.

## Logging

- **Types of Logs**: There are different types of logs, including application logs, system logs, security logs, and audit logs. Each type of log records specific information related to the operation of the software or system.
- **Log Levels**: Logs are typically classified into different levels, such as INFO, DEBUG, WARN, ERROR, and FATAL, based on their severity. Each log level indicates the importance of the message and helps to filter and analyze logs.
- **Log Formats**: Logs can be formatted in different ways, such as plain text, JSON, XML, or structured data. The log format determines how the log messages are stored and processed by logging tools.
- **Log Rotation**: Log rotation is the process of managing log files to prevent them from growing too large. Log rotation tools archive old log files, compress them, and delete outdated logs to free up disk space.
- **Log Aggregation**: Log aggregation tools collect logs from multiple sources, such as servers, applications, and network devices, and store them in a centralized location. Log aggregation helps to analyze logs, search for patterns, and troubleshoot issues across the entire system.

## Monitoring

- **Key Metrics**: Monitoring tools collect data on key metrics, such as response time, CPU usage, memory usage, disk space, network traffic, and error rates. These metrics help to assess the performance and availability of the system.
- **Alerting**: Monitoring tools provide alerting capabilities to notify administrators when key metrics exceed predefined thresholds. Alerts can be sent via email, SMS, or other communication channels to ensure that issues are addressed promptly.
- **Dashboards**: Monitoring tools generate dashboards that display real-time and historical data on key metrics. Dashboards provide a visual representation of system performance and help administrators to identify trends and anomalies.
- **Trending and Analysis**: Monitoring tools analyze historical data to identify trends, patterns, and anomalies in system performance. Trending and analysis help to optimize resource utilization, predict capacity requirements, and troubleshoot recurring issues.
- **Integration**: Monitoring tools can be integrated with other systems, such as logging tools, ticketing systems, and automation tools, to streamline operations and improve efficiency.

## Logging and Monitoring Tools

- **Logging Tools**: Some popular logging tools include Elasticsearch, Logstash, Kibana (ELK Stack), Splunk, Graylog, and Fluentd. These tools provide features for collecting, storing, analyzing, and visualizing logs.
- **Monitoring Tools**: Some popular monitoring tools include Prometheus, Grafana, Nagios, Zabbix, Datadog, and New Relic. These tools provide features for collecting, visualizing, and alerting on key metrics.

# API Design

## API Paradigms

- REST:

  - stateless: each request from a client to a server must contain all the information necessary to understand the request, and cannot take advantage of any stored context on the server.
  - client-server: the client and server are separate entities that communicate over a stateless protocol, such as HTTP.
  - json/xml: REST APIs typically use JSON or XML as the data format for requests and responses.
  - uniform interface: REST APIs use a uniform interface, such as HTTP methods (GET, POST, PUT, DELETE) and resource identifiers (URLs), to interact with resources.
  - caching: REST APIs can use caching to improve performance by storing responses and reusing them for subsequent requests.

- GraphQL:

  - query language: GraphQL is a query language for APIs that allows clients to request only the data they need.
  - single endpoint: GraphQL APIs typically expose a single endpoint that clients can use to query and mutate data (always POST request).
  - type system: GraphQL APIs define a type system that describes the data available in the API and the relationships between different types.
  - introspection: GraphQL APIs support introspection, which allows clients to query the schema and discover available types and fields.
  - real-time updates: GraphQL APIs can support real-time updates using subscriptions, which allow clients to receive updates when data changes.

- gRPC:
  - protocol buffers: gRPC uses Protocol Buffers as the interface definition language (IDL) for defining services and message types.
  - HTTP/2: gRPC uses HTTP/2 as the transport protocol, which provides features such as multiplexing, flow control, and header compression.
  - bidirectional streaming: gRPC supports bidirectional streaming, which allows clients and servers to send multiple messages in both directions.
  - code generation: gRPC generates client and server code from the service definition, which makes it easy to build and maintain APIs in multiple languages.
  - performance: gRPC is designed for high-performance communication between services and supports features such as connection pooling and load balancing.

## API Design Best Practices

- **Use Descriptive URLs**: Use descriptive URLs that represent resources and actions in a meaningful way. For example, use `/users` to represent a collection of users and `/users/{id}` to represent a specific user.
- **Use HTTP Methods**: Use HTTP methods (GET, POST, PUT, DELETE) to perform actions on resources. For example, use GET to retrieve data, POST to create data, PUT to update data, and DELETE to delete data.
- **Versioning**: Use versioning to manage changes to the API over time. For example, use `/v1/users` to represent version 1 of the users API and `/v2/users` to represent version 2.
- **Error Handling**: Use standard HTTP status codes (2xx, 3xx, 4xx, 5xx) to indicate the status of the request. Provide meaningful error messages and details in the response body to help clients troubleshoot issues.
- **Security**: Use authentication and authorization mechanisms, such as API keys, OAuth, and JWT, to secure access to the API. Use HTTPS to encrypt data in transit and prevent
- **Rate Limiting**: Use rate limiting to prevent abuse of the API and ensure fair usage. Limit the number of requests per client, per IP address, or per API key to prevent denial-of-service attacks.
- **CORS**: Use Cross-Origin Resource Sharing (CORS) to allow web applications to make requests to the API from different domains. Configure CORS headers to specify which domains are allowed to access the API.
- **Documentation**: Provide comprehensive documentation for the API, including details on resources, actions, parameters, and response formats. Use tools such as Swagger and OpenAPI to generate interactive API documentation.
- **Testing**: Test the API thoroughly using automated tests, such as unit tests, integration tests, and end-to-end tests. Use tools such as Postman and Newman to automate
- **Monitoring**: Monitor the API performance, availability, and usage metrics to identify issues and optimize performance. Use tools such as Prometheus and Grafana to collect and visualize metrics.

# Caching

- Caching is the process of storing data in a temporary storage area, called a cache, to reduce the time it takes to access the data. Caching can improve performance, reduce latency, and lower the load on backend systems by serving cached data to clients instead of fetching it from the original source.

## Types of Caching

- **Client-Side Caching**: Client-side caching stores data on the client side, such as in the browser's cache or local storage. This type of caching can improve performance by reducing the number of requests to the server and minimizing data transfer.
- **Server-Side Caching**: Server-side caching stores data on the server side, such as in memory, disk, or a distributed cache. This type of caching can improve performance by reducing the time it takes to retrieve data from the backend system.
- **Content Delivery Network (CDN)**: A CDN is a distributed network of servers that caches content, such as images, videos, and static files, at edge locations close to users. CDNs can improve performance by serving cached content from the nearest edge location to reduce latency.
- **Database Caching**: Database caching stores query results, data objects, or database records in memory to reduce the time it takes to retrieve data from the database. This type of caching can improve performance by serving cached data instead of executing expensive queries.
- **Application Caching**: Application caching stores computed or processed data in memory to reduce the time it takes to generate the data. This type of caching can improve performance by serving cached data instead of recalculating it.

## Caching Strategies

- **Time-Based Caching**: Time-based caching stores data in the cache for a specific period of time, such as 5 minutes or 1 hour. This strategy can improve performance by serving cached data until it expires and is refreshed.
- **TTL-Based Caching**: TTL-based caching stores data in the cache with a Time-To-Live (TTL) value that specifies how long the data should be cached. This strategy can improve performance by expiring cached data after a specific duration.
- **Cache Invalidation**: Cache invalidation removes or updates cached data when the original data changes. This strategy can improve performance by ensuring that clients receive up-to-date data from the cache.
- **Cache-Control Headers**: Cache-Control headers specify caching directives, such as public, private, no-cache, no-store, max-age, and s-maxage, to control how data is cached by clients and proxies. This strategy can improve performance by optimizing caching behavior.
- **Cache Key Design**: Cache key design specifies how data is stored and retrieved in the cache. This strategy can improve performance by ensuring that data is cached efficiently and retrieved quickly.

## Caching Best Practices

- **Identify Hotspots**: Identify performance bottlenecks and frequently accessed data that can benefit from caching. Use monitoring tools to analyze traffic patterns and identify hotspots.
- **Use Cache Headers**: Use Cache-Control headers to specify caching directives, such as max-age, s-maxage, public, private, no-cache, and no-store, to control how data is cached by clients and proxies.
- **Set Cache-Control Headers**: Set Cache-Control headers in HTTP responses to control caching behavior. Use directives such as max-age, s-maxage, public, private, no-cache, and no-store to specify caching rules.
- **Use ETags**: Use ETags (Entity Tags) to validate cached data and ensure that clients receive up-to-date content. ETags are unique identifiers that can be used to compare cached data with the original data.
- **Cache Invalidation**: Implement cache invalidation strategies to remove or update cached data when the original data changes. Use cache keys, cache tags, or cache keys to identify and invalidate cached data.
- **Use CDN**: Use a Content Delivery Network (CDN) to cache content, such as images, videos, and static files, at edge locations close to users. CDNs can improve performance by serving cached content from the nearest edge location.
- **Monitor Cache Performance**: Monitor cache performance, hit rate, miss rate, and cache size to optimize caching behavior. Use monitoring tools to collect and analyze cache metrics and identify areas for improvement.
- **Use Distributed Cache**: Use a distributed cache, such as Redis or Memcached, to store data in memory and improve performance. Distributed caches can scale horizontally and provide high availability and fault tolerance.
